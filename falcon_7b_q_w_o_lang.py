# -*- coding: utf-8 -*-
"""Falcon_7b_Q_without.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kieZwZcm4fcUa9f8aAejBmC5qef-FaYg

# Problem Statement : Asking Question to the falcon-7b-instruct Chatbot (with and without langchain)
"""



"""# Importing and installing dependencies"""

!pip install transformers accelerate einops

from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = "tiiuae/falcon-7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model)

"""# Transformers pipeline - text generation"""

pipeline = transformers.pipeline(
    "text-generation",
    model = model,
    tokenizer = tokenizer,
    torch_dtype = torch.bfloat16,
    trust_remote_code = True,
    device_map = "auto",
)

"""# Model Inference"""

prompt = " What is the difference between falcon-7b large language model and ChatGPT? "

sequences = pipeline(prompt,
                     max_length = 400,
                     do_sample = True,
                     top_k = 10,
                     num_return_sequences = 1,
                     eos_token_id = tokenizer.eos_token_id
                     )

"""# print the response"""

for seq in sequences:
  print(f"Result: {seq['generated_text']}")

